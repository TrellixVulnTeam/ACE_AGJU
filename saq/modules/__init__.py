# vim: sw=4:ts=4:et:cc=120

import datetime
import inspect
import json
import logging
import os
import os.path
import re
import signal
import sys
import threading
import time
import urllib.parse
import xml.etree.ElementTree as ET

import saq

from saq.analysis import Analysis, Observable, MODULE_PATH, SPLIT_MODULE_PATH
from saq.constants import *
from saq.error import report_exception
from saq.network_semaphore import NetworkSemaphoreClient
from saq.splunk import SplunkQueryObject
from saq.util import create_timedelta, parse_event_time, create_directory, atomic_open


import pytz
import requests

class WatchedFile(object):
    last_mtime = 0
    path = None
    callback = None

    def __init__(self, path, callback):
        self.path = path
        self.callback = callback

class AnalysisModule(object):
    """The base class of all analysis logic.  All your custom analysis modules extend this class."""

    def __init__(self, config_section, *args, **kwargs):
        super().__init__(*args, **kwargs)
        assert isinstance(config_section, str)

        # the section in the configuration that applies to this analysis module
        self.config_section = config_section
        self.config_section_name = config_section[len('analysis_module_'):]
        self.config = None
        self._load_config()

        # the instance defines the specific instance of the given analysis module
        # some analysis modules can have multiple instances
        # which are basically analysis modules that share the same python code but have different configurations
        # for example, a SplunkQueryAnalysisModule might have different instances for the different splunk searches
        # you might want to run
        # if this value is missing then it defaults to None, which is the "default" instance
        self.instance = self.config.get('instance', None)

        # a refernce to the RootAnalysis object we're analyzing
        self.root = None

        # a reference to the engine this module is running out of
        self.engine = None

        # the actual semaphore to use
        self.semaphore = None

        # we'll keep track of the Analysis and Observable objects we've generated
        # this is useful for cleanup routines
        self.generated_analysis = []
        self.generated_observables = []

        # observables that are excluded from being analyzed by this module
        self.observable_exclusions = {} # key = o_type, value = [] of o_value
        self.load_exclusions()

        # observables that are excluded from being generated by this module
        self.expected_observables = {} # key = o_type, value = set(o_value)
        self.load_expected_observables()

        # something might try to cancel an analysis execution
        self.cancel_analysis_flag = False

        # sometimes a module can depend on another service that is failing
        # when that happens we can trigger "cooldown periods" where we skip executing this module until some time
        # has elapsed

        # the time at which the cooldown expires (None if no cooldown is in effect)
        self.cooldown_timeout = None

        # a list (set) of files that are currently being watched
        self.watched_files = {} # key = path, value = WatchedFile

        # set to true if this is a threaded module
        self.is_threaded = False
        if 'threaded' in self.config:
            self.is_threaded = self.config.getboolean('threaded')

        # how often self.execute_threaded is called from the self.execute_threaded_loop
        self.threaded_execution_frequency = 1
        if 'threaded_execution_frequency' in self.config:
            self.threaded_execution_frequency = self.config.getint('threaded_execution_frequency')

        # the actual thread performing the work
        self.threaded_execution_thread = None

        # event to signal the thread can stop
        self.threaded_execution_stop_event = None

        # the priority of the analysis module
        # lower priority scores go first
        # higher priority scores go last
        self.priority = self.config.getint('priority', fallback=10)

        # the next time we check to see if any files we are watching have changed
        # this is automatically checked and updated every time this module is used to analyze something
        self.next_check_watched_files = None

        # is this a module that groups analysis of duplicate values by time?
        self.observation_grouping_time_range = None
        if 'observation_grouping_time_range' in self.config:
            self.observation_grouping_time_range = create_timedelta(self.config['observation_grouping_time_range'])

        # automation limit settings control how many times an analysis module runs automatically during correlation
        self.automation_limit = self.config.getint('automation_limit', fallback=None)

        # control how long execution is allowed to take (in seconds) before the parent process kills it
        self.maximum_analysis_time = self.config.getint('maximum_analysis_time', fallback=saq.CONFIG['global'].getint('maximum_analysis_time'))

    @property
    def is_grouped_by_time(self):
        """Returns True if the observation_grouping_time_range configuration option is being used."""
        return self.observation_grouping_time_range is not None

    def start_threaded_execution(self):
        if not self.is_threaded:
            return

        self.threaded_execution_stop_event = threading.Event()
        self.threaded_execution_thread = threading.Thread(target=self.execute_threaded_loop_wrapper,
                                                          name="Threaded Module {}".format(self))
        self.threaded_execution_thread.start()
        logging.info("started thread {}".format(self.threaded_execution_thread))

    def stop_threaded_execution(self):
        if not self.is_threaded:
            return

        logging.info("stopping threaded execution for {}".format(self))

        self.threaded_execution_stop_event.set()
        start = datetime.datetime.now()
        while True:
            self.threaded_execution_thread.join(5)
            if not self.threaded_execution_thread.is_alive():
                break

            logging.error("thread {} is not stopping".format(self.threaded_execution_thread))

            # have we been waiting for a really long time?
            if (datetime.datetime.now() - start).total_seconds() >= saq.EXECUTION_THREAD_LONG_TIMEOUT:
                logging.critical("execution thread {} is failing to stop - process dying".format(
                                  self.threaded_execution_thread))
                # suicide
                os._exit(1)

        logging.debug("threaded execution module {} has stopped ({})".format(self, self.threaded_execution_thread))

    def reset(self):
        """Resets the module to initialized state."""
        self.root = None
        self.semaphore = None
        self.generated_analysis = []
        self.generated_observables = []
        self.cancel_analysis_flag = False
        self.cooldown_timeout = None

    @property
    def cooldown_period(self):
        """Number of seconds this module stays in cooldown state.  Defaults to 60."""
        if 'cooldown_period' in self.config:
            return self.config.getint('cooldown_period')

        # if not defined then we default to 60 second cooldown
        return 60

    @property
    def semaphore_name(self):
        """The semaphore this module uses.  Defaults to None (no semaphore is used.)"""
        if 'semaphore' in self.config:
            return self.config['semaphore']

        return None

    def _load_config(self):
        # has the configuration not changed?
        if self.config is saq.CONFIG[self.config_section]:
            return

        # reference to the configuration object itself
        self.config = saq.CONFIG[self.config_section]
        self.load_config()

    def load_config(self):
        """Called when saq.CONFIG changes."""
        pass

    # TODO use a modern library or method to do this
    def watch_file(self, path, callback):
        """Watches the given file and executes callback when it detects that the file has been modified."""
        if path in self.watched_files:
            logging.warning("replacing callback {} for {} with {}".format(self.watched_files[path], path, callback))

        logging.debug("watching file {}".format(path))
        self.watched_files[path] = WatchedFile(path, callback)
        # go ahead and load it up
        self.check_watched_files()

    def check_watched_files(self):
        # is it time to check the files this module is watching?
        if self.next_check_watched_files is not None and datetime.datetime.now() < self.next_check_watched_files:
            return

        # check every 5 seconds
        self.next_check_watched_files = datetime.datetime.now() + \
                                        datetime.timedelta(seconds=saq.CONFIG['global'].getint(
                                                           'check_watched_files_frequency'))

        for watched_file in self.watched_files.values():
            try:
                if not os.path.exists(watched_file.path):
                    continue
                
                current_mtime = os.stat(watched_file.path).st_mtime
                if watched_file.last_mtime != current_mtime:
                    if watched_file.last_mtime != 0:
                        logging.info("detected change to {}".format(watched_file.path))

                    watched_file.last_mtime = current_mtime

                    try:
                        watched_file.callback()
                    except Exception as e:
                        logging.error("callback failed for {} file {}: {}".format(self, watched_file.path, e))
                        report_exception()

            except Exception as e:
                logging.error("unable to check file {}: {}".format(watched_file.path, e))
                report_exception()

    def verify_environment(self):
        """Called after module is loaded to verify that everything it needs exists.."""
        pass

    def verify_config_exists(self, config_name):
        """Verifies the given configuration exists for this module.  Use this from verify_environment."""
        try:
            _ = self.config[config_name]
        except KeyError:
            raise KeyError("module {} missing configuration item {}".format(self, config_name))

    def verify_config_item_has_value(self, config_key):
        """Verifies the given configuration exists and has a value. Use this from verify_environment."""
        self.verify_config_exists(config_key)
        if not self.config[config_key]:
            raise TypeError("module {} cofiguration item {} is not defined.".format(self, config_key))

    def verify_path_exists(self, path):
        """Verifies the given path exists.  If the path is relative then it is relative to SAQ_HOME."""
        _path = path
        if not os.path.isabs(path):
            _path = os.path.join(saq.SAQ_HOME, path)

        if not os.path.exists(_path):
            raise RuntimeError("missing {} used by {}".format(path, self))

    def verify_program_exists(self, path):
        """Verifies the given program exists on the system.  If relative then $PATH is checked using which."""
        from subprocess import Popen, DEVNULL

        if os.path.isabs(path):
            if not os.path.exists(path):
                raise RuntimeError("missing {} used by {}".format(path, self))
        else:
            p = Popen(['which', path], stdout=DEVNULL, stderr=DEVNULL)
            p.wait()

            if p.returncode:
                raise RuntimeError("cannot find {} used by {}".format(path, self))

    def create_required_directory(self, dir):
        """Creates the given directory if it does not already exist.  Relative paths are relative to DATA_DIR."""

        if not os.path.isabs(dir):
            dir = os.path.join(saq.DATA_DIR, dir)
            
        if os.path.isdir(dir):
            return

        try:
            logging.debug("creating required directory {}".format(dir))
            os.makedirs(dir)
        except Exception as e:
            if not os.path.isdir(dir):
                logging.error("unable to create required directory {} for {}: {}".format(dir, self, e))
                raise e

    @property
    def name(self):
        result = self.config_section[len('analysis_module_'):]
        if self.instance is not None:
            result += f':{self.instance}'

        return result

    @property
    def shutdown(self):
        """Returns True if the current analysis engine is shutting down, False otherwise."""
        return self.engine.shutdown

    @property
    def controlled_shutdown(self):
        return self.engine.controlled_shutdown

    def sleep(self, seconds):
        """Utility function to sleep for N seconds without blocking shutdown."""
        while not self.shutdown and not self.cancel_analysis_flag and seconds > 0:
            # we also want to support sleeping for less than a second
            time.sleep(1 if seconds > 0 else seconds)
            seconds -= 1
        
    @property
    def state(self):
        """Returns the dict object you can use to maintain state over time."""
        try:
            return self.root.state[self.name]
        except KeyError:
            return None

    @state.setter
    def state(self, value):
        self.root.state[self.name] = value

    def initialize_state(self, value={}):
        """Sets the state for this module to the given value (defaults to empty dict.)
           If the state is already set this function does nothing."""
        if self.name not in self.root.state:
            self.root.state[self.name] = value

    def wait_for_analysis(self, observable, analysis_type, instance=None):
        """Waits for the given Analysis (by type) be available for the given Observable."""
        from saq.engine import WaitForAnalysisException

        assert isinstance(observable, Observable)
        assert inspect.isclass(analysis_type) and issubclass(analysis_type, Analysis)
        assert instance is None or isinstance(instance, str)

        # do we already have a dependency here?
        dep = observable.get_dependency(MODULE_PATH(analysis_type, instance=instance))
        
        # have we already analyzed this observable for this analysis type?
        analysis = observable.get_analysis(analysis_type, instance=instance)
        
        # if the dependency has been completed or resolved then we just return whatever we got
        # even if it was nothing
        if dep and ( dep.completed or dep.resolved or dep.failed ):
            return analysis

        # if we haven't analyzed this yet or we have and it hasn't completed yet (delayed) then we wait
        if analysis is None or isinstance(analysis, Analysis) and not analysis.completed:
            raise WaitForAnalysisException(observable, analysis_type, instance=instance)

        # otherwise we return the analysis
        return analysis

    def enter_cooldown(self):
        """Puts this module into cooldown mode which will cause it to get skipped for self.cooldown_period seconds."""
        self.cooldown_timeout = datetime.datetime.now() + datetime.timedelta(seconds=self.cooldown_period)
        logging.warning("{} entered cooldown period until {}".format(self, self.cooldown_timeout))

    def add_observable_exclusion(self, o_type, o_value):
        if o_type not in self.observable_exclusions:
            self.observable_exclusions[o_type] = []

        if o_value not in self.observable_exclusions[o_type]:
            self.observable_exclusions[o_type].append(o_value)
            #logging.debug("loaded observable exclusion type {} value {} for {}".format(
                #o_type, o_value, self))

    def is_excluded(self, observable):
        """Returns True if the given observable is excluded from analysis for this module."""
        if observable.type not in self.observable_exclusions:
            return False

        for exclusion_value in self.observable_exclusions[observable.type]:
            if observable.matches(exclusion_value):
                return True

        return False

    def load_exclusions(self):
        # load any observable exclusions for this module
        self.observable_exclusions = {}
        for key in self.config.keys():
            if key.startswith("exclude_"):
                o_type, o_value = self.config[key].split(':', 1)
                if o_type == 'observable_group':
                    # load exclusion from observable group
                    logging.debug("loading exclusion list from observable group {} for {}".format(o_value, self))
                    config_key = 'observable_group_{}'.format(o_value)
                    for key in saq.CONFIG[config_key].keys():
                        if key.startswith('define_'):
                            o_type, o_value = saq.CONFIG[config_key][key].split(':', 1)
                            self.add_observable_exclusion(o_type, o_value)
                else:
                    self.add_observable_exclusion(o_type, o_value)

        # append global exclusions to this list
        for option_name in saq.CONFIG['observable_exclusions'].keys():
            o_type, o_value = saq.CONFIG['observable_exclusions'][option_name].split(':', 1)
            self.add_observable_exclusion(o_type, o_value)

    def add_expected_observable(self, o_type, o_value):
        """Adds the given observable as an expected observable of this module.
           Expected observables are never generated during analysis."""
        if o_type not in self.expected_observables:
            self.expected_observables[o_type] = set()

        self.expected_observables[o_type].add(o_value)
        logging.debug("loaded expected observable type {} value {} for {}".format(
            o_type, o_value, self))

    def is_expected_observable(self, o_type, o_value):
        """Returns True if the given observable is an expected observable for this module."""
        try:
            return o_value in self.expected_observables[o_type]
        except KeyError:
            logging.debug("o_type {} not in {}".format(o_type, self))
            return False

    def load_expected_observables(self):
        self.expected_observables = {}
        for key in self.config.keys():
            if key.startswith("expect_"):
                o_type, o_value = self.config[key].split(':', 1)
                self.add_expected_observable(o_type, o_value)

    @property
    def generated_analysis_type(self):
        """Returns the type of the Analysis-based class this AnalysisModule generates.  
           Returns None if this AnalysisModule does not generate an Analysis object."""
        return None

    def create_analysis(self, observable):
        """Initializes and adds the generated Analysis for this module to the given Observable. 
           Returns the generated Analysis."""
        # have we already created analysis for this observable?
        if self.generated_analysis_type is None:
            logging.critical("called create_analysis on {} which does not actually create Analysis".format(self))
            return None

        analysis = observable.get_analysis(self.generated_analysis_type, instance=self.instance)
        if analysis:
            logging.debug("returning existing analysis {} in call to create analysis from {} for {}".format(
                          analysis, self, observable))
            return analysis
        
        # otherwise we create and initialize a new one
        analysis = self.generated_analysis_type()
        analysis.instance = self.instance
        analysis.initialize_details()
        observable.add_analysis(analysis)
        return analysis

    # XXX this is not supported at all
    @property
    def valid_analysis_target_type(self):
        """Returns a valid analysis target type for this module.  
           Defaults to Observable.  Return None to disable the check."""
        return Observable

    @property
    def valid_observable_types(self):
        """Returns a single (or list of) Observable type that are valid for this module.  
           If the configuration setting valid_observable_types is present then those values are used.
           Defaults to None (all types are valid.)  Return None to disable the check."""

        if 'valid_observable_types' not in self.config:
            return None

        return [_.strip() for _ in self.config['valid_observable_types'].split(',')]

    @property
    def valid_queues(self):
        """Returns a list of strings that are valid queues for this module.  
           If the configuration setting valid_queues is present then those values are used.
           Defaults to None (all queues are valid.)  Return None to disable the check."""

        if 'valid_queues' not in self.config:
            return None

        return [_.strip() for _ in self.config['valid_queues'].split(',')]

    @property
    def invalid_queues(self):
        """Returns a list of strings that are invalid queues for this module.  
           If the configuration setting invalid_queues is present then those values are used.
           Defaults to None (no queues are invalid)  Return None to disable the check."""

        if 'invalid_queues' not in self.config:
            return None

        return [_.strip() for _ in self.config['invalid_queues'].split(',')]

    @property
    def required_directives(self):
        """Returns a list of required directives for the analysis to occur. 
           If the configuration setting required_directives is present, then those values are used.
           Defaults to an empty list."""

        if 'required_directives' not in self.config:
            return []

        return [_.strip() for _ in self.config['required_directives'].split(',')]

    @property
    def required_tags(self):
        """Returns a list of required tags for the analysis to occur. 
           If the configuration setting required_tags is present, then those values are used.
           Defaults to an empty list."""

        if 'required_tags' not in self.config:
            return []

        return [_.strip() for _ in self.config['required_tags'].split(',')]

    @property
    def cache(self):
        return self.config.getboolean('cache') if 'cache' in self.config else False

    @property
    def version(self):
        return self.config.getint('version') if 'version' in self.config else 1

    @property
    def cache_expiration(self):
        return create_timedelta(self.config['cache_expiration']) if 'cache_expiration' in self.config else datetime.timedelta(hours=24)

    def load_cached_analysis(self, observable):
        try:
            # do not load cached analysis if caching is not enabled
            if not self.cache:
                return False

            # path to cached analysis
            path = os.path.join(saq.SAQ_HOME, saq.DATA_DIR, 'analysis_cache', f'{observable.cache_id}.{self.generated_analysis_type.__name__}.v{self.version}.json')

            # return false if there is no cached analysis at the cached analysis path
            if not os.path.isfile(path):
                return False

            # do not use cached analysis if it is expired
            if os.path.getmtime(path) + self.cache_expiration.total_seconds() < time.time():
                return False

            # create analysis from cached analysis
            with atomic_open(path) as f:
                cached_analysis = json.load(f)
                analysis = self.create_analysis(observable)
                analysis.details = cached_analysis['details']
                for cached_observable in cached_analysis['observables']:
                    analysis.add_observable(cached_observable['type'], cached_observable['value'])

        # do not use cacheed analysis if it fails to load for any reason
        except Exception as e:
            return False

        # cached analysis successfully loaded
        return True

    def cache_analysis(self, observable):
        try:
            base_path = os.path.join(saq.SAQ_HOME, saq.DATA_DIR, "analysis_cache")
            create_directory(base_path)
            path = os.path.join(base_path, f'{observable.cache_id}.{self.generated_analysis_type.__name__}.v{self.version}.json')
            with atomic_open(path, 'w') as f:
                analysis = observable.get_analysis(self.generated_analysis_type, instance=self.instance)
                cached_analysis = {
                    "details": analysis.details,
                    "observables": [ {"type":o.type, "value":o.value} for o in analysis.observables ]
                }
                json.dump(cached_analysis, f)

        except Exception as e:
            logging.warn(f"failed to cache analysis: {e}")

    def custom_requirement(self, observable):
        """Optional function is called as an additional check to see if this observalbe should be
           analyzed by this module. Returns True if it should be, False if not.
           If this function is not overridden then it is ignored."""
        raise NotImplementedError()

    def accepts(self, obj):
        """Returns True if this object should be analyzed by this module, False otherwise."""

        # we still call execution on the module in cooldown mode
        # there may be things it can (or should) do while on cooldown

        # if this analysis module does not generate analysis then we can skip this
        # these are typically analysis modules that only do pre or post analysis work
        if self.generated_analysis_type is None:
            return False

        if self.valid_analysis_target_type is not None:
            if not isinstance(obj, self.valid_analysis_target_type):
                logging.debug("{} is not a valid target type for {}".format(obj, self))
                return False

        # XXX these isinstance checks are from an older version ace that tried to support analyzing analysis modules
        # XXX these can probably be removed
        if isinstance(obj, Observable) and self.valid_observable_types is not None:
            # a little hack to allow valid_observable_types to return a single value
            valid_types = self.valid_observable_types
            if isinstance(valid_types, str):
                valid_types = [valid_types]

            try:
                if obj.type not in valid_types:
                    #logging.debug("{} is not a valid type for {}".format(obj.type, self))
                    return False
            except Exception as e:
                logging.error("valid_observable_types returned invalid data type {} for {}".format(
                    type(valid_types), self))
                return False

        # do not accept observables from queues we are not configured to accept
        if isinstance(obj, Observable) and self.valid_queues is not None and obj.root is not None and obj.root.queue not in self.valid_queues:
            return False

        # do not accept observables from queues we are configured to ignore
        if isinstance(obj, Observable) and self.invalid_queues is not None and obj.root is not None and obj.root.queue in self.invalid_queues:
            return False

        if isinstance(obj, Observable):
            # does this analysis module exclude this observable from analysis?
            if self.is_excluded(obj):
                #logging.debug("observable {} is excluded from analysis by {}".format(obj, self))
                return False

            # does this observable exclude itself from this kind of analysis?
            if obj.is_excluded(self):
                #logging.debug("analysis module {} excluded from analyzing {}".format(self, obj))
                return False

            # does this analysis module require directives?
            for directive in self.required_directives:
                if not obj.has_directive(directive):
                    #logging.debug("{} does not have required directive {} for {}".format(obj, directive, self))
                    return False

            # does this analysis module require tags?
            for tag in self.required_tags:
                if not obj.has_tag(tag):
                    #logging.debug("{} does not have required directive {} for {}".format(obj, directive, self))
                    return False

            # does the module have a custom requirement routine defined?
            try:
                if not self.custom_requirement(obj):
                    logging.debug(f"{obj} does not pass custom requirements for {self}")
                    return False
            except NotImplementedError:
                pass

            # have we already generated analysis for this target?
            current_analysis = obj.get_analysis(self.generated_analysis_type, instance=self.instance)
            if current_analysis is not None:
                # did it return nothing?
                if isinstance(current_analysis, bool) and not current_analysis:
                    logging.debug("already analyzed {} with {} and returned nothing".format(obj, self))
                    return False

                # has this analysis completed?
                if current_analysis.completed:
                    logging.debug("already analyzed {} with {}".format(obj, self))
                    return False

        # are we in cooldown mode?
        # XXX side effect!
        if self.cooldown_timeout:
            # are we still in cooldown mode?
            if datetime.datetime.now() < self.cooldown_timeout:
                logging.debug("{} in cooldown mode".format(self))
            else:
                self.cooldown_timeout = None
                logging.info("{} exited cooldown mode".format(self))

        # does this module have automation limits?
        if self.automation_limit is not None:
            # and is this observable NOT ignoring automation limits?
            # this can be the case if an analyst is forcing analysis of something
            if not obj.has_directive(DIRECTIVE_IGNORE_AUTOMATION_LIMITS):
                # how many times have we already generated analysis with this module?
                current_analysis_count = len(self.root.get_analysis_by_type(self.generated_analysis_type))
                if current_analysis_count >= self.automation_limit:
                    logging.debug(f"{self} reached automation limit of {self.automation_limit} for {self.root}")
                    return False

        # end with custom logic, which defaults to True if not implemented
        return self.should_analyze(obj)

    def __str__(self):
        result = type(self).__name__
        if self.instance is not None:
            result += f':{self.instance}'

        return result

    def cancel_analysis(self):
        """Try to cancel the analysis loop."""
        self.cancel_analysis_flag = True
        
        # also try to cancel a semaphore acquire request, if there is one
        # XXX there's certainly a race condition here
        if self.semaphore is not None:
            self.semaphore.cancel_request()

        # execute any custom handlers defined by the engine
        self.cancel_analysis_handler()

    def cancel_analysis_handler(self):
        """Override this function to implement custom cancel code."""
        pass

    def acquire_semaphore(self):
        """Wait for the semaphore to become available, or return immediately if this module does not use a semaphore."""
        if self.semaphore_name is None:
            logging.warning("semaphore name is None for {}".format(self))
            return False

        if self.cancel_analysis_flag:
            logging.info("called acquire_semaphore() for module {} but cancel_analysis_flag is set".format(self))
            return False

        # semaphores can be globally disabled
        if not saq.SEMAPHORES_ENABLED:
            logging.debug("semaphores are disabled - not using semaphore {}".format(self.semaphore))
            return False

        # create a new semaphore client to use
        self.semaphore = NetworkSemaphoreClient()

        logging.debug("analysis module {0} acquiring semaphore {1}".format(self, self.semaphore_name))
        try:
            if not self.semaphore.acquire(self.semaphore_name):
                raise RuntimeError("acquire returned False")
            #logging.debug("analysis module {0} acquired semaphore {1}".format(self, self.semaphore_name))
        except Exception as e:
            logging.error("unable to acquire semaphore {} : {}".format(self.semaphore_name, e))
            report_exception()

            self.semaphore = None

            # TODO fall back to something else we can use
            return False

        return True

    def release_semaphore(self):
        """Release the acquired semaphore, or do nothing if this module does not use a semaphore."""
        if self.semaphore is None:
            return

        try:
            self.semaphore.release()
            #logging.debug("analysis module {0} released semaphore {1}".format(self, self.semaphore_name))
        except Exception as e:
            logging.error("unable to release semaphore: {0}".format(str(e)))
            report_exception()

    def analyze(self, obj, final_analysis=False):
        """Called by an analysis engine to analyze a given Analysis or Observable object."""

        assert isinstance(obj, Analysis) or isinstance(obj, Observable)

        # if we're watching any files, see if they've changed and need to be reloaded
        self.check_watched_files()

        if isinstance(obj, Observable):
            if self.analysis_covered(obj):
                logging.debug(f"{obj} is already covered by another {self} analysis")
                return False

        # try to load analysis from cache first
        if self.load_cached_analysis(obj):
            analysis_result = True
        else:
            # if we are executing in "final analysis mode" then we call this function instead
            if final_analysis:
                analysis_result = self.execute_final_analysis(obj)
            else:
                analysis_result = self.execute_analysis(obj)

            # cache analysis if enabled
            if analysis_result and self.cache:
                self.cache_analysis(obj)

        # if we are grouping by time then we mark this Observable as a future target for other grouping
        # (if we got an analysis result)
        if analysis_result and self.is_grouped_by_time:
            obj.grouping_target = True

        return analysis_result

    def cleanup(self):
        """Called after all analysis has completed. Override this if you need to clean up something after analysis."""
        pass

    def delay_analysis(self, observable, analysis, hours=None, minutes=None, seconds=None,
                       timeout_hours=None, timeout_minutes=None, timeout_seconds=None):
        """Called to delay this analysis until the specified amount of time has expired."""
        if hours is None and minutes is None and seconds is None:
            hours = 0
            minutes = 0
            seconds = 10

        if hours is None:
            hours = 0

        if minutes is None:
            minutes = 0

        if seconds is None:
            seconds = 0

        logging.debug("adding delayed analysis for "
                      "{} by {} on {} analysis {} hours {} minutes {} seconds {}".format(
                      self.root, self, observable, analysis, hours, minutes, seconds))

        if self.engine.delay_analysis(self.root, observable, analysis, self, 
                                      hours=hours, minutes=minutes, seconds=seconds,
                                      timeout_hours=timeout_hours, 
                                      timeout_minutes=timeout_minutes, 
                                      timeout_seconds=timeout_seconds):
            analysis.completed = False
            analysis.delayed = True
            return True

        analysis.completed = True
        analysis.delayed = False
        return False

    # THIS is what you override in your subclass!
    def execute_analysis(self, analysis):
        """Called to analyze Analysis or Observable objects. Override this in your subclass.
           Return True if analysis executed successfully (even if nothing was done.)
           Return False if analysis should not occur for this target.
           Returning False will cause the engine to skip any further analysis by this module for this target."""
        raise NotImplemented()

    def execute_final_analysis(self, analysis):
        """Called to analyze Analysis or Observable objects after all other analysis has completed."""
        return False

    def execute_pre_analysis(self):
        """This is called once at the very beginning of analysis."""
        pass

    def execute_post_analysis(self):
        """This is called after all analysis work has been performed and no outstanding work is left.
           If the function returns False then the function can possibly get called again if the analysis mode changes.
           If the function returns True then the function will not get called again."""
        return True

    def execute_threaded(self):
        """This is called on a thread if the module is configured as threaded."""
        pass

    def execute_threaded_loop_wrapper(self):
        try:
            self.execute_threaded_loop()
        finally:
            # make sure we remove the session if we created one
            saq.db.remove()

    def execute_threaded_loop(self):
        # continue to execute until analysis has completed
        while True:
            try:
                self.execute_threaded()
            except Exception as e:
                logging.error("{} failed threaded execution on {}: {}".format(self, self.root, e))
                report_exception()
                return

            # wait for self.threaded_execution_frequency seconds before we execute again
            # make sure we exit when we're asked to
            timeout = self.threaded_execution_frequency
            while not self.engine.cancel_analysis_flag and \
                  not self.threaded_execution_stop_event.is_set() and \
                  timeout > 0:

                time.sleep(1)
                timeout -= 1

            if self.engine.cancel_analysis_flag:
                return

            if self.threaded_execution_stop_event.is_set():
                return

    def auto_reload(self):
        """Called every N seconds (see auto_reload_frequency in abstract
engine) in the main process to allow the module to update or change
configuration."""
        return

    def should_analyze(self, obj):
        """Put your custom "should I analyze this?" logic in this function."""
        return True

    def execute_maintenance(self):
        """Override this function to provide some kind of maintenance routine that is called every
           maintenance_frequency seconds."""
        pass

    @property
    def maintenance_frequency(self):
        """Returns how often to execute the maintenance function, in seconds, or None to disable (the default.)"""
        return None

    def analysis_covered(self, observable):
        """Returns True if the value of this observable has already been analyzed in another observable
           that has an observation time with range of this observable."""

        # for this to have any meaning, the observations must have correponding times
        if not observable.time:
            return False

        # is this feature enabled for this analysis module?
        if not self.is_grouped_by_time:
            return False

        start_time = observable.time - self.observation_grouping_time_range
        end_time = observable.time + self.observation_grouping_time_range

        grouping_target_available = False

        # NOTE that we also iterate over the observable we're looking at
        for target_observable in self.root.get_observables_by_type(observable.type):

            if target_observable.value != observable.value:
                continue

            # does this target observables time fall in the range we're looking for?
            if target_observable.time is None:
                continue

            if target_observable.time >= start_time and target_observable.time <= end_time:
                # does this target_observable already have this analysis generated?
                if target_observable.get_analysis(self.generated_analysis_type, instance=self.instance):
                    logging.debug(f"{target_observable} already has analysis for "
                                  f"{self.generated_analysis_type} between times {start_time} and {end_time} "
                                  f"{observable}")
                    return True

                # this target is in range AND is already a grouping target
                # NOTE that we want to keep looking for existing analysis so we don't break out of the loop here
                if target_observable.grouping_target:
                    logging.debug(f"{target_observable} detected as grouping target for "
                                  f"{self.generated_analysis_type} {observable}")
                    grouping_target_available = True

        # if we didn't find anything and the observable we're looking at is a grouping target then this is
        # the one we want to analyze
        if observable.grouping_target:
            logging.debug(f"using {observable} as grouping target for {self.generated_analysis_type}")
            return False

        # if we didn't find anything but we did find another observable in the group that is already a grouping
        # target then we are considered "covered" because *that* observable will get the analysis
        if grouping_target_available:
            return True

        # otherwise we analyze this one
        return False

class TagAnalysisModule(AnalysisModule):
    """These types of modules ignore any exclusion rules."""
    def load_exclusions(self):
        pass

    def is_excluded(self, observable):
        return False 

def splunktime_to_datetime(splunk_time):
    """Convert a splunk time in 2015-02-19T09:50:49.000-05:00 format to a datetime object."""
    assert isinstance(splunk_time, str)
    #return datetime.datetime.strptime(splunk_time.split('.')[0], '%Y-%m-%dT%H:%M:%S')
    return parse_event_time(splunk_time)

def splunktime_to_saqtime(splunk_time):
    """Convert a splunk time in 2015-02-19T09:50:49.000-05:00 format to SAQ time format YYYY-MM-DD HH:MM:SS."""
    assert isinstance(splunk_time, str)
    return parse_event_time(splunk_time).strftime(event_time_format_json_tz)

    #m = re.match(r'^([0-9]{4})-([0-9]{2})-([0-9]{2})T([0-9]{2}):([0-9]{2}):([0-9]{2})\.[0-9]{3}[-+][0-9]{2}:[0-9]{2}$', splunk_time)
    #if not m:
        #logging.error("_time field does not match expected format: {0}".format(splunk_time))
        #return None

   # reformat this time for SAQ
    #return '{0}-{1}-{2} {3}:{4}:{5}'.format(
        #m.group(1),
        #m.group(2),
        #m.group(3),
        #m.group(4),
        #m.group(5),
        #m.group(6))

class SplunkAnalysisModule(AnalysisModule, SplunkQueryObject):
    """An analysis module that uses Splunk."""
    
    def __init__(self, *args, **kwargs):
        super(SplunkAnalysisModule, self).__init__(*args, **kwargs)

        # load the splunk settings from configuration file
        self.enabled = saq.CONFIG.get('splunk', 'enabled')
        self.uri = saq.CONFIG.get('splunk', 'uri')
        self.username = saq.CONFIG.get('splunk', 'username')
        self.password = saq.CONFIG.get('splunk', 'password')
        self.max_result_count = saq.CONFIG.get('splunk', 'max_result_count')

        # a splunk-based module can define it's own relative time frame for event collection
        if saq.CONFIG.has_option(self.config_section, 'relative_duration_before'):
            self.relative_duration_before = saq.CONFIG.get(self.config_section, 'relative_duration_before')
        else:
            self.relative_duration_before = saq.CONFIG.get('splunk', 'relative_duration_before')

        if saq.CONFIG.has_option(self.config_section, 'relative_duration_after'):
            self.relative_duration_after = saq.CONFIG.get(self.config_section, 'relative_duration_after')
        else:
            self.relative_duration_after = saq.CONFIG.get('splunk', 'relative_duration_after')

    @property
    def semaphore_name(self):
        return 'splunk'

    def splunk_query(self, *args, **kwargs):
        try:
            if not self.acquire_semaphore():
                if not self.cancel_analysis_flag:
                    logging.warning("unable to acquire semaphore")
                return None

            return self.splunk_query_exec(*args, **kwargs)

        finally:
            self.release_semaphore()

    def splunk_query_exec(self, query, event_time=None):
        assert event_time is None or isinstance(event_time, datetime.datetime)
        if not self.enabled:
            return False

        # if no time range is specified then we default to the configured time relative 
        # to the event_time assuming the source of the analysis is an Alert

        if event_time is None:
            logging.debug("no time specified using alert time")
            event_time = self.root.event_time_datetime

        return self.query_relative(query, event_time)

    def handle_cancel_event(self):
        # try to stop any existing splunk query
        self.cancel()

class CarbonBlackAnalysisModule(AnalysisModule):
    """An analysis module that directly queries Carbon Black servers as part of its analysis."""
    @property
    def cb(self):
        if not hasattr(self, '_cb'):
            from cbapi import CbEnterpriseResponseAPI
            self._cb = CbEnterpriseResponseAPI(credential_file=os.path.join(saq.SAQ_HOME, 
                                               saq.CONFIG['carbon_black']['credential_file']))

        return self._cb

class ELKAnalysisModule(AnalysisModule):
    """An analysis module that queries ElasticSearch as part of its analysis."""
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        
        # for relative time searches, how far back and forward do we go?
        self.earliest_timedelta = create_timedelta(saq.CONFIG['elk']['relative_duration_before'])
        if 'relative_duration_before' in self.config:
            self.earliest_timedelta = create_timedelta(self.config['relative_duration_before'])

        self.latest_timedelta = create_timedelta(saq.CONFIG['elk']['relative_duration_after'])
        if 'relative_duration_after' in self.config:
            self.latest_timedelta = create_timedelta(self.config['relative_duration_after'])

        # format the elk search uri with the username and password if it's specified
        if saq.CONFIG['elk']['username'] and saq.CONFIG['elk']['password']:
            # using urlencoding in case username or password has funky characters
            self.elk_uri = 'https://{}:{}@{}'.format(urllib.parse.quote_plus(saq.CONFIG['elk']['username']), 
                                                     urllib.parse.quote_plus(saq.CONFIG['elk']['password']), 
                                                     saq.CONFIG['elk']['uri'])
        else:
            self.elk_uri = 'https://{}'.format(saq.CONFIG['elk']['uri'])

        # make sure it ends with /
        if not self.elk_uri.endswith('/'):
            self.elk_uri += '/'

        # the maximum number of results we would want
        self.max_result_count = saq.CONFIG['elk'].getint('max_result_count')
        if 'max_result_count' in self.config:
            self.max_result_count = self.config.getint('max_result_count')

        # if we've specified a cluster in the global config then we prefix our index with that cluster
        self.cluster = '' # by default we don't specify the cluster at all
        if saq.CONFIG['elk']['cluster']:
            self.cluster = saq.CONFIG['elk']['cluster']

        # we can also specify the cluster for this specific module
        if 'cluster' in self.config:
            self.cluster = self.config['cluster']

    def search(self, index, query, target=None, earliest=None, latest=None, fields=[], sort=[]):
        """Searches ELK using the given query.
            :param index: The index to search.
            :param query: The query to execute against the index.
            :param target: A datetime to reference if the time span is relative.
            :param earliest: The earliest part of an absolute time span.
            :param latest: The latest part of an absolute time span.
            :param fields: Optional list of fields in include in the results. Defaults to all fields.
            :param sort: Optional list of JSON dicts specifying the sort. Defaults to event_timestamp desc.
            (see https://www.elastic.co/guide/en/elasticsearch/reference/current/search-request-sort.html)
            :returns: or None on failure
        """

        # is elk searching enabled?
        if not saq.CONFIG['elk'].getboolean('enabled'):
            logging.warning("analysis module {} enabled but elk is disabled globally".format(self.name))
            return None

        if ( earliest is None and latest is not None ) or ( earliest is not None and latest is None ):
            raise RuntimeError("if you pass an absolute time range to ELKAnalysisModule.search you must "
                             "provide values for both earliest and latest parameters")

        if earliest is None and latest is None: 
            if target is None and self.root is None:
                raise RuntimeError("no time specified for ELKAnalysisModule.search and no root object to pull a time from")

            # if no time is specified then use the insert_date of the root analysis object
            if target is None:
                target = self.root.event_time_datetime

            earliest = target - self.earliest_timedelta
            latest = target + self.latest_timedelta

        # if we don't have a timezone set then we assume the timezone is whatever the local system timezone is
        if earliest.tzinfo is None:
            earliest = earliest.astimezone().astimezone(pytz.UTC)

        if latest.tzinfo is None:
            latest = latest.astimezone().astimezone(pytz.UTC)

        # if we did not specify a sort then default to sorting by event_timestamp desc
        if not sort:
            sort = [ { 'event_timestamp': { 'order': 'desc'} } ]

        search_json = {
            'size': self.max_result_count,
            'query': {
                'bool': {
                    'filter': [
                    {
                        'query_string': {
                            'query': query,
                        }
                    },
                    {
                        'range': {
                            '@timestamp': {
                                'format': 'epoch_second',
                                'gte': earliest.timestamp(),
                                'lte': latest.timestamp(),
                            }
                        }
                    },
                    ]
                }
            },
            'sort': sort,
        }

        # are we limiting what fields we get back?
        if fields:
            search_json['_source'] = fields

        # are we specifying a cluster?
        if self.cluster:
            index = '{}:{}'.format(self.cluster, index)

        search_uri = "{}{}/_search".format(self.elk_uri, index)
        search_id = '{} query {} earliest {} latest {}'.format(search_uri, query, earliest, latest)
        logging.debug("executing search {}".format(search_id))

        headers = {'Content-type':'application/json'}
        search_result = requests.get(search_uri, data=json.dumps(search_json), headers=headers, verify=False) # XXX remove verify=False
        if search_result.status_code != 200:
            logging.warning("search failed for {}: {}".format(search_id, search_result.text))
            return None

        json_result = search_result.json()
        logging.debug("search result {}: timed_out {} took {} shards {} clusters {}".format(
                      search_id,
                      json_result['timed_out'],
                      json_result['took'],
                      json_result['_shards'],
                      json_result['_clusters']))

        return json_result

class ExternalProcessAnalysisModule(AnalysisModule):
    """An analysis module that executes an external process as part of it's analysis."""
    
    def __init__(self, *args, **kwargs):
        super(ExternalProcessAnalysisModule, self).__init__(*args, **kwargs)
        
        # reference to the Popen command result
        self.external_process = None

    def handle_cancel_event(self):
        # kill the external process
        if self.external_process is not None:
            logging.debug("terminating external process {0}".format(self.external_process))
            try:
                os.killpg(self.external_process.pid, signal.SIGTERM)
                self.external_process.wait(5)
            except:
                logging.debug("killing external process {0}".format(self.external_process))
                try:
                    os.killpg(self.external_process.pid, signal.SIGKILL)
                    self.external_process.wait()
                except Exception as e:
                    logging.debug("unable to kill external process {0}: {1}".format(self.external_process, str(e)))
                    #report_exception()
